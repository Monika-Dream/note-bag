2022 / 10 / 8
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- çˆ¬è™«å¸¸ç”¨è§£æä¿¡æ¯æ–¹å¼
result_data.text		è¿”å›å‡ºæ–‡æœ¬
result_data.json()		è¿”å›å‡º JSON æ•°æ®, å¸¸å¸¸å¯¼å…¥ import json 
result_data.content		å¸¸ç”¨äºè¾“å‡ºå›¾ç‰‡ä¿¡æ¯,è¿”å›äºŒè¿›åˆ¶å½¢å¼çš„å›¾ç‰‡æ•°æ®
response.status_code	å“åº”çŠ¶æ€ç 
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- çˆ¬è™«å¸¸ç”¨è¯·æ±‚å¤´ä¿¡æ¯

 - User-Agent: è¯·æ±‚è½½ä½“çš„èº«ä»½æ ‡è¯†
 - Connection: è¯·æ±‚å®Œæ¯•å, æ˜¯æ–­å¼€è¿æ¥è¿˜æ˜¯ä¿æŒè¿æ¥

----------------------------------------------------------- çˆ¬è™«å¸¸ç”¨å“åº”å¤´ä¿¡æ¯

 - Content-Type: æœåŠ¡å™¨å“åº”å›å®¢æˆ·ç«¯çš„æ•°æ®ç±»å‹

----------------------------------------------------------- ç½‘ç«™å®‰å…¨åŠ å¯†æ–¹å¼

 - å¯¹ç§°å¯†é’¥åŠ å¯†
 - éå¯¹ç§°å¯†é’¥åŠ å¯†
 - è¯ä¹¦å¯†é’¥åŠ å¯†		<- https æ‰€é‡‡ç”¨æ–¹å¼

----------------------------------------------------------- requests çˆ¬è™«æ¨¡å—

 - æ¨¡æ‹Ÿæµè§ˆå™¨å‘é€è¯·æ±‚
	 - æŒ‡å®š url
	 - å‘èµ·è¯·æ±‚	 get/post
	 - è·å–å“åº”æ•°æ®
	 - æŒä¹…åŒ–å­˜å‚¨
	 - æ— æ³•å¼‚æ­¥è¯·æ±‚

----------------------------------------------------------- requests çˆ¬è™«æ¨¡å—çˆ¬å– qingju.ga é¡µé¢

import requests

if __name__ == '__main__':
    						# æŒ‡å®š URL
    url = "https://qingju.ga/"
    						# requests å‘èµ· Get è¯·æ±‚, ä¼šè¿”å›ä¸€ä¸ªå“åº”å¯¹è±¡
    response = requests.get(url=url)
    						# æŸ¥çœ‹å“åº”çš„ä¿¡æ¯å†…å®¹(æºç æ•°æ®)
    page_text = response.text
    with open("./Galgameç½‘ç«™æºç .html","wb+") as file:
        file.write(page_text.encode())
    print("é¡µé¢çˆ¬å–ç»“æŸ")

----------------------------------------------------------- å®ç°ç®€æ˜“ç½‘é¡µé‡‡é›†å™¨

 - UAä¼ªè£…: è®©çˆ¬è™«å¯¹åº”çš„è¯·æ±‚è½½ä½“èº«ä»½æ ‡è¯†ä¼ªè£…ä¸ºæŸä¸€æ¬¾æµè§ˆå™¨

import requests

if __name__ == '__main__':
    					# TODO ä¿®æ”¹ User-Agent ä¼ªè£…ä¸ºæµè§ˆå™¨
    headers = {
        'User-Agent': "Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 106.0.0.0Safari / 537.36Edg / 106.0.1370.37"
    }
    					# TODO è¯·æ±‚ URL
    url = "https://www.sogou.com/web"
    					# TODO è¯·æ±‚ä½“
    search = input("è¯·è¾“å…¥è¦æœç´¢çš„å†…å®¹")
    					# TODO å¤„ç† URL æ‰€å¸¦å‚æ•°
    param = {
        "query": search
    }
    response = requests.get(url=url, params=param, headers=headers)
    result_data = response.text
    with open("./è‡ªå®šä¹‰ç½‘é¡µè¯·æ±‚.html","bw") as file:
        file.write(result_data.encode())

----------------------------------------------------------- ç ´è§£ç™¾åº¦ç¿»è¯‘

import requests
import json

if __name__ == '__main__':
    url = "https://fanyi.baidu.com/sug"
    user = {
        'User-Agent': "Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 106.0.0.0Safari / 537.36Edg / 106.0.1370.37"
    }
    change_text = input("è¯·è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ ä¸­->è‹± \n")
    query = {
        "kw": change_text
    }
    response = requests.post(url=url,data=query)
    dic_obj = response.json()
    path = open("./dog.json","w",encoding="utf-8")
        #ä¸­æ–‡ä¸èƒ½ä½¿ç”¨ ensure_ascii ç¼–ç , æ‰€ä»¥è¦å˜ False
    json.dump(dic_obj,path,ensure_ascii=False)
    path.close()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- æ•°æ®è§£ææ“ä½œ

 - èšç„¦çˆ¬è™«
 - æ­£åˆ™
 - bs4
 - xpath

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4å¤§è‡´ä»‹ç»

 - æ˜¯ Python ä¸­ç‹¬æœ‰çš„è§£ææ–¹å¼, æ— æ³•ä½¿ç”¨åœ¨å…¶ä»–è¯­è¨€ä¹‹ä¸­
 - è§£æåŸç†
	 - å®ä¾‹åŒ–ä¸€ä¸ª BeautifulSoup ä¸­, å¹¶å°†é¡µé¢æºç æ•°æ®åŠ è½½åˆ°è¯¥å¯¹è±¡ä¸­
	 - é€šè¿‡è°ƒç”¨ BeautifulSoup ä¸­ç›¸å…³çš„å±æ€§ä¸æ–¹æ³•è¿›è¡Œæ ‡ç­¾å®šä½å’Œæ•°æ®æå–
 - ç¯å¢ƒå®‰è£…
	 - pip install bs4
	 - pip install lxml
 - å¦‚ä½•ä½¿ç”¨
	 - from bs4 import BeautifulSoup
	 - BeautifulSoup(htmlæ–‡æ¡£,"lxml")

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4ä½¿ç”¨

 - 1.å®šä½ç¬¬ä¸€æ¬¡åœ¨é¡µé¢å‡ºç°æ ‡ç­¾åŠæ ‡ç­¾å†…å®¹ä¸€å¹¶è¿”å›			      .TagName
from bs4 import BeautifulSoup
soup = BeautifulSoup(htmlæ–‡æ¡£,"lxml")
print(soup.æ ‡ç­¾åç§°)				#å®šä½å‡ºæ‰€æœ‰æ ‡ç­¾ä¸º[  ]çš„ 


 - 2.å®šä½ç¬¬ä¸€æ¬¡æ ‡ç­¾åä¸º [ ] çš„æ ‡ç­¾åŠæ ‡ç­¾å†…å®¹ä¸€å¹¶è¿”å›			      .find( )
	 - class_ ä»…ä»…æ˜¯å› ä¸ºå…³é”®å­—
from bs4 import BeautifulSoup
soup = BeautifulSoup(htmlæ–‡æ¡£,"lxml")
print(soup.find( 'a' ))			#å®šä½å‡ºç¬¬ä¸€æ¬¡æ ‡ç­¾ä¸º a çš„ 
print(soup.find( 'a', class_=' song ' ))		#å®šä½å‡ºç¬¬ä¸€æ¬¡æ ‡ç­¾ä¸º a çš„, ä¸”å±æ€§ä¸ºclass = 'song'


 - 3.æŸ¥æ‰¾æ‰€æœ‰æ ‡ç­¾åä¸º [ ] çš„æ ‡ç­¾åŠæ ‡ç­¾å†…å®¹ä¸€å¹¶è¿”å›( åˆ—è¡¨ )		      .find_all( )
from bs4 import BeautifulSoup
soup = BeautifulSoup(htmlæ–‡æ¡£,"lxml")
print(soup.find_all( 'a' ))			#æŸ¥æ‰¾å‡ºæ‰€æœ‰æ ‡ç­¾ä¸º a çš„ 
print(soup.find_all( 'a', class_=' song ' ))	#æŸ¥æ‰¾å‡ºç¬¬ä¸€æ¬¡æ ‡ç­¾ä¸º a çš„, ä¸”å±æ€§ä¸ºclass = 'song'


 - 4.ä½¿ç”¨é€‰æ‹©å™¨æ¥å®šä½å‡ºæ ‡ç­¾å…ƒç´ 					      .select( )
from bs4 import BeautifulSoup
soup = BeautifulSoup(htmlæ–‡æ¡£,"lxml")
print(soup.select( '.TagName' ))		#ä½¿ç”¨é€‰æ‹©å™¨æ¥å®šä½å‡ºæ ‡ç­¾å…ƒç´ 	

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4 é’ˆå¯¹å…ƒç´ æå–ä¹‹åçš„ä¸€äº›é’ˆå¯¹æå–å‡½æ•°

 - è¿™é‡Œå°†ç”¨ result_data æ¥ä»£è¡¨ soup.find( 'a' ) è¿™äº›çš„å¸¦æ ‡ç­¾è¿”å›å€¼	æå–æ–‡æœ¬å†…å®¹
1.result_data.text		è·å–æ ‡ç­¾ä¸­çš„æ‰€æœ‰å†…å®¹
2.result_data.get_text()	è·å–æ ‡ç­¾ä¸­çš„æ‰€æœ‰å†…å®¹
3.result_data.string		åªå¯ä»¥è·å–è¯¥æ ‡ç­¾ä¸‹é¢ç›´ç³»çš„æ ‡ç­¾å†…å®¹( ä¸ä¼šå»è·å–å­é›†æ–‡æœ¬ )

 - è¿™é‡ŒæŒç»­å°†ç”¨ result_data æ¥ä»£è¡¨ soup.find( 'a' ) è¿™äº›çš„å¸¦æ ‡ç­¾è¿”å›å€¼	æå–å±æ€§å†…å®¹
1.result_data.a[ 'href' ]	è·å– a æ ‡ç­¾ä¸­ href å±æ€§å†…å®¹

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- Xpathå¤§è‡´ä»‹ç»

 - å®ä¾‹åŒ–ä¸€ä¸ª etree çš„å¯¹è±¡, ä¸”éœ€è¦å°†éœ€è¦è§£æçš„é¡µé¢åŠ è½½åˆ°è¯¥å¯¹è±¡ä¸­
 - è°ƒç”¨ etree å¯¹è±¡ä¸­çš„ xpath æ–¹æ³•ç»“åˆç€ xpath è¡¨è¾¾å¼å®ç°æ ‡ç­¾çš„å®šä½ä¸å†…å®¹çš„æ•è·
 - éœ€è¦å®‰è£…ç¯å¢ƒ 	pip install lxml
 - æ‹¥æœ‰ä¸¤ç§ä½¿ç”¨æ–¹æ³•
	 - æœ¬åœ°è§£æ
		form lxml import etree
		etree.parse(filePath)
	 - ç½‘ç»œè§£æ
		form lxml import etree
		etree.HTML('page_text')

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- Xpathä½¿ç”¨

 - / æ ‡è¯†ä¸‹ä¸€ä¸ªå±‚çº§, éœ€è¦å†™å‡ºä¸‹ä¸€ä¸ªå±‚çº§åç§°
 - // æ ‡è¯†ä¸‹ä¸€ä¸ªå±‚çº§, ä¹Ÿè¡¨ç¤ºä»ä»»æ„ä½ç½®æ‰¾åˆ°

1.xpath è¡¨è¾¾å¼ä¾æ—§æ˜¯ä¾é ç€å±‚çº§å…³ç³»è¿›è¡Œå®šä½
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title")

2.xpath å±æ€§é€‰æ‹©å†™æ³•
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title[@class='song']")

3.xpath ç´¢å¼•é€‰æ‹©å†™æ³•		ä»ä¸€å¼€å§‹
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title[@class='song']/a[3]")

4.å¤šæ¬¡é€‰å–
//div[@class='download-url']/a[1]/@href | //h1[@class='title']/text()
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- xpath é’ˆå¯¹å…ƒç´ æå–ä¹‹åçš„ä¸€äº›é’ˆå¯¹æå–å‡½æ•°

1.å–æ–‡æœ¬å†…å®¹
 - ç›´æ¥åœ¨è·¯å¾„åé¢å†™ text() å³å¯, è¿”å›åˆ—è¡¨, å¦‚éœ€è·å–éç›´ç³»æ–‡æœ¬, åªéœ€å°† /text() å˜ //text()
tree.xpath("/html/head/title[@class='song']/a[3]/text()")

2.å–å±æ€§å†…å®¹

tree.xpath("/html/head/title[@class='song']/a[3]/@href")

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- è¶…çº§ğŸ¦…è¯†åˆ«éªŒè¯ç å›¾ç‰‡æ–¹å¼

 - æ‰“å¼€å·¥å…·åŒ…
è´¦å· Monika
å¯†ç  sxhmzz954578
è½¯ä»¶ID:		939969	
è½¯ä»¶è¯´æ˜:		è½¯ä»¶KEY:	ea37b44e02accbd58f11239f7d096658


----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- ä½¿ç”¨ ddddcoræ¨¡å— è¯†åˆ«éªŒè¯ç 

 - pip install ddddocr
 - è¯·å»ç›¸å…³æ–‡æ¡£æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- å…³äºæ‹¥æœ‰ Cookie ç½‘ç«™æ€ä¹ˆæ¨¡æ‹Ÿç™»é™†

 - ä½¿ç”¨ session ä¼šè¯å¯¹è±¡
	 - 1.å¯ä»¥è¿›è¡Œè¯·æ±‚çš„å‘é€
	 - 2.å¦‚æœè¯·æ±‚è¿‡ç¨‹ä¸­äº§ç”Ÿäº† Cookie å€¼, åˆ™è¯¥ Cookie ä¼šè¢«è‡ªåŠ¨å­˜å‚¨/æºå¸¦åœ¨è¯¥ session å¯¹è±¡ä¸­
		 - åˆ›å»ºä¸€ä¸ª session å¯¹è±¡
			 - session = requests.Session( )
		 - ä½¿ç”¨ session å¯¹è±¡è¿›è¡Œæ¨¡æ‹Ÿç™»é™† post è¯·æ±‚çš„å‘é€ (cookie ä¼šè¢«å­˜å‚¨åˆ° session ä¸­)
		 - session å¯¹è±¡å¯¹ä¸ªäººä¸»é¡µå¯¹åº”çš„ get è¯·æ±‚è¿›è¡Œå‘é€ ( æºå¸¦äº†cookie )
 - åŒºåˆ«
	 - 1.requests æ¯æ¬¡éƒ½æ˜¯å•ç‹¬è¯·æ±‚
	 - 2.session å¯ä»¥å‚¨å­˜ä¸Šæ¬¡è¯·æ±‚çš„æœåŠ¡å™¨ã€€cookie

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- session ä½¿ç”¨

session = requests.Session( )
session.get()
session.post()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- ä»£ç†

 - ä»£ç†çš„ä½œç”¨
	 - çªç ´è‡ªèº« IP è®¿é—®é™åˆ¶
	 - éšè—è‡ªèº«çš„çœŸå® IP
 - ä»£ç†ç›¸å…³çš„ç½‘ç«™
	 - å¿«ä»£ç†
	 - www.goubanjia.com

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- å¼‚æ­¥çˆ¬è™«

 - å¤šçº¿ç¨‹ / å¤šè¿›ç¨‹
	 - å¥½å¤„: å•ç‹¬ä¸ºé˜»å¡çš„æ“ä½œå•ç‹¬å¼€å¯çº¿ç¨‹æˆ–è€…è¿›ç¨‹, é˜»å¡æ“ä½œå°±å¯ä»¥å¼‚æ­¥æ‰§è¡Œ
	 - åå¤„: æ— æ³•æ— é™åˆ¶çš„å¼€å¯å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹
 - çº¿ç¨‹æ±  / è¿›ç¨‹æ± 
	 - å¥½å¤„: å¯ä»¥é™ä½ç³»ç»Ÿå¯¹è¿›ç¨‹ä¸çº¿ç¨‹é”€æ¯çš„é¢‘ç‡, é™ä½ç³»ç»Ÿçš„å¼€é”€
	 - åå¤„: æ± ä¸­èƒ½å¼€çš„æ‹¥æœ‰ä¸Šé™

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- è¿›ç¨‹æ± å¼‚æ­¥çˆ¬è™«( ä»¥åè¯·ä½¿ç”¨ä¸‹é¢ä¸€è¯¾çš„æ–¹å¼ )

1.å¯è¿­ä»£å¯¹è±¡ä¸­éœ€è¦æ”¾å…¥ æ•°ç»„ ä¹‹ç±»çš„å½¢ä¼¼çš„ç±»å‹ åº•å±‚æœ‰ __item__ çš„
2.ä½¿ç”¨ä¸Šä¹‹åå·®ä¸å¤šè·Ÿ for è¯­å¥ä¸€æ ·	

 - å¯¼å…¥è¿›ç¨‹æ± 
from multiprocessing.dummy import Pool
 - å®ä¾‹åŒ–ç±» ä¸ åˆ›å»ºå¤šå°‘ä¸ªè¿›ç¨‹
pool = Pool( 4 )
 - ä½¿ç”¨
pool.map(è¦æ‰§è¡Œçš„å‡½æ•°å, å¯è¿­ä»£å¯¹è±¡)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- çº¿ç¨‹æ± å¼‚æ­¥çˆ¬è™«
from concurrent.futures import ThreadPoolExecutor / ProcessPoolExecutor       çº¿ç¨‹æ±  / è¿›ç¨‹æ± 
with ThreadPoolExecutor(20) as t:
    for i in ["www.baidu.com",...]
        t.submit(å‡½æ•°å,MusicArr=i)        <- ä¼ å‚æ–¹å¼

 - ä¸€æ ·çš„ä½¿ç”¨æ–¹æ³•

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- åç¨‹å­¦ä¹ 

 - éœ€è¦å€ŸåŠ© asyncio æ¨¡å—æ‰§è¡Œ åç¨‹å¯¹è±¡
import asyncio

async def fun():			#è¿”å›åç¨‹å¯¹è±¡
    print("Hello Monika")
    #time.sleep( 4 )			å‡ºç°åŒæ­¥æ“ä½œçš„æ—¶å€™, å¼‚æ­¥å°±ä¸­æ–­äº†	
    await asyncio.sleep(4)		#å¼‚æ­¥è€—æ—¶æ“ä½œ, éœ€è¦æŒ‚èµ·
    print("Hello Monika End")

async def fun1():
    print("Hello Sayori")
    await asyncio.sleep(2)
    print("Hello Sayori End")


async def main():
    tasks = [
       asyncio.create_task(fun()) ,	#éœ€è¦åŒ…è£¹ asyncio.create_task()
       asyncio.create_task(fun1())
    ]
    await asyncio.wait(tasks)

if __name__ == '__main__':
        asyncio.run(main())


tasks = [  asyncio.create_task(download( url )) for url in urls  ]

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp è¯»å–æ–¹å¼ç®€ä»‹( è§£æä¿¡æ¯æ–¹å¼ )

 - æ•°æ®äºŒè¿›åˆ¶è§£æ
response.content.read()             <==>    response.content

 - æ•°æ®æ–‡æœ¬åŒ–
response.text()                     <==>    response.text

 - æ•°æ® JSON åŒ–
response.json()                     <==>    response.json()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp æ¨¡å—ä½¿ç”¨( ä¸€æ¬¡å®Œæ•´çš„å•çº¿ç¨‹å¼‚æ­¥ä»£ç  )

import asyncio,aiohttp

urls = [
    "http://kr.shanghai-jiuxin.com/file/bizhi/20221017/fwfia1fs4ex.jpg",
    "http://kr.shanghai-jiuxin.com/file/bizhi/20221017/soszuvwqtai.jpg",
    "https://www.umei.cc/d/file/20221010/10ddc39143559f50acc19e4d90405f87.jpg"
]

async def download(url):
    name = url.rsplit("/",1)[1]
    print(name)
    print("æ­£åœ¨è¯·æ±‚",url)
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
        async with session.get(url=url) as response:
            with open(f"./åç¨‹çˆ¬å–æµ‹è¯•/{name}","wb") as file:
                file.write(await response.content.read())
    print("æˆåŠŸ")

async def main():
    tasks = []
    for url in urls:
        tasks.append(asyncio.create_task(download(url)))
    await asyncio.wait(tasks)

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp æ³¨æ„äº‹é¡¹

1.æŠ¥é”™: RuntimeError: Event loop is closed 
 - ä½¿ç”¨   loop = asyncio.get_event_loop()   loop.run_until_complete(main()) å³å¯
 - ä½†æ˜¯ä½¿ç”¨ asyncio.run(main()) ä¹Ÿè¡Œ, æŠ¥é”™ä¸å½±å“ç»“æœ
 - ç›®å‰æŸ¥é˜…åˆ°æ˜¯å› ä¸º asyncio.run() è‡ªåŠ¨æå‰å…³é—­äº† "ä»»åŠ¡é˜Ÿåˆ—", ä¸Šé¢æ²¡å‡ºé”™çš„åŸå› æ˜¯æ‰‹åŠ¨æŒ¡,è¿˜å°‘äº†è¡Œ loop.close() ä»£ç 


2.æŠ¥é”™: aiohttp.client_exceptions.ClientConnectorCertificateError: 
    Cannot connect to host www.umei.cc:443 ssl:True [SSLCertVerificationError: 
        (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)')]
 - åœ¨ async with aiohttp.ClientSession() as session: ä¸­æ·»åŠ  connector=aiohttp.TCPConnector(ssl=False) å³å¯
 - async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
 - ç›®å‰æŸ¥é˜…çš„èµ„æ–™æŒ‡å‘ https è¯ä¹¦çš„ä¸€ç³»åˆ—é—®é¢˜

3.æŠ¥é”™: "ä¿¡å·ç¯è¶…æ—¶æ—¶é—´å·²åˆ°" ä»€ä¹ˆæ„æ€ï¼Ÿ
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session: 
    é¢‘ç¹åˆ›å»º, æ³¨æ„å¤ç”¨

4.æŠ¥é”™: aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected





----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiofiles ä½¿ç”¨

 - å¼‚æ­¥çš„å†™å…¥æ–‡ä»¶
 async with aiofiles.open("è·¯å¾„",mode="w",encoding="utf-8") as file:
    await file.write(await response.content.read())

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium ç¯å¢ƒå®‰è£…

 - 1.ç¯å¢ƒå®‰è£…
 pip install selenium
 ä¸‹è½½æµè§ˆå™¨é©±åŠ¨ ( æ³¨æ„å¯¹åº”ç‰ˆæœ¬ )
 http://chromedriver.storage.googleapis.com/index.html      <- è°·æ­Œ
 
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium ä½¿ç”¨
        
from selenium.webdriver import Chrome
from selenium.webdriver.common.keys import Keys             <- é”®ç›˜äº‹ä»¶
from selenium.webdriver.support.select import Select        <- select ä¸‹æ‹‰èœå•æ“ä½œ
from selenium
web = Chrome()
web.get("ç½‘å€")                                             <- æ‰“å¼€ç½‘ç«™


----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium å±æ€§

 - é€‰ä¸­å…ƒç´ åçš„è¿”å›ç»“æœå¯ä»¥åŠ  .text è¾“å‡º
web.get("ç½‘å€")                                  æ‰“å¼€æµè§ˆå™¨
web.close()                                     å…³é—­å½“å‰çª—å£
web.title                                       ç½‘ç«™åç§°
element = web.find_element(by='xpath',value='//*[@id="changeCityBox"]/p[1]/a')  æŸ¥æ‰¾å±æ€§
element.send_keys("å†…å®¹",Keys.ENTER)             è¾“å…¥å†…å®¹å¹¶ä¸”å›è½¦, æ³¨æ„å¯¼å…¥æ¨¡å—
element.click()                                 ç‚¹å‡»å±æ€§
web.implicitly_wait(3)                          ç­‰å¾…
web.switch_to.window(web.window_handles[-1])    å°† selenium æ³¨æ„åŠ›åˆ‡æ¢åˆ°æœ€åä¸€ä¸ªçª—å£
web.switch_to.default_content()                 å›åˆ‡åˆ°é»˜è®¤çª—å£

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium æ³¨æ„

1. - åœ¨ç‚¹å‡»è¶…é“¾æ¥åˆ°æ–°å¢åŠ çš„é¡µé¢æ—¶, selenium æ³¨æ„åŠ›è¿˜åœ¨ä¸Šä¸ªçª—å£ä¹‹ä¸­, éœ€è¦
     - web.switch_to.window(web.window_handles[-1])    å°† selenium æ³¨æ„åŠ›åˆ‡æ¢åˆ°æœ€åä¸€ä¸ªçª—å£
 - åœ¨ web.close() å½“å‰çª—å£æ—¶, åŒä¸Š
     - web.switch_to.window(web.window_handles[0])

2. - iframe å†…å®¹è·å–æ–¹å¼
     - å®šä½åˆ°å…ƒç´ 
     - web.switch_to.frame(å…ƒç´ )

3. select å¦‚ä½•é€‰æ‹©å†…å®¹
     - å¯¼å…¥ from selenium.webdriver.support.select import Select 
     - å®šä½åˆ°ä¸‹æ‹‰èœå•
     - sel = Select(ä¸Šé¢è¿”å›çš„å…ƒç´ )
        - sel.select_by_index()                 æ ¹æ® ç´¢å¼•               åˆ‡æ¢
        - sel.select_by_index()                 æ ¹æ® Valueå€¼            åˆ‡æ¢
        - sel.select_by_visible_text()          æ ¹æ® ä½ æ‰€çœ‹åˆ°çš„æ ‡ç­¾å†…å®¹  åˆ‡æ¢

4. è¢«æ£€æµ‹åˆ°äº†æ€ä¹ˆåŠ(æµè§ˆå™¨å¤§äºæˆ–ç­‰äº 88 ,ä¸”é’ˆå¯¹è°·æ­Œæµè§ˆå™¨çš„æ–¹æ¡ˆ)
     - from selenium.webdriver.chrome.options import Options
     - opt = Options()
     - opt.add_argument("--disable-blink-features=AutomationControlled")
     - web = Chrome(options=opt)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium è®¾ç½®æ— å¤´æµè§ˆå™¨

 - å°±æ˜¯è®©æµè§ˆå™¨åœ¨åå°è¿è¡Œ
from selenium.webdriver.chrome.options import Options
opt = Options()
opt.add_argument("--headless")
opt.add_argument("--disable-gpu")
web = Chrome(opt)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium å¤„ç†å›¾åƒä¿¡æ¯

 - å®šä½å…ƒç´ 
 img = web.find_element(by='xpath',value='//*[@id="changeCityBox"]/p[1]/a').screenshot_as_png
     - è¿”å›å›¾ç‰‡å­—èŠ‚,å¯ä»¥ä¾› è¶…çº§ğŸ¦… ç›´æ¥ä½¿ç”¨

 - å¦‚æœæ˜¯éœ€è¦ç‚¹å‡»çš„å›¾ç‰‡
     - å¯¼åŒ… from selenium.webdriver.common.action_chains import ActionChains      <- äº‹ä»¶é“¾
     - ActionChains(web).move_to_element_with_offset(å›¾ç‰‡element,x,y).click().perform()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium å¤„ç†æ‹–æ‹½éªŒè¯

 - å¯¼åŒ… from selenium.webdriver.common.action_chains import ActionChains
 - ActionChains(web).drag_and_drop_by_offset(å…ƒç´ element,x,y).perform()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy æ¡†æ¶ç¯å¢ƒæ­å»º

 - pip install -i https://pypi.tuna.tsinghua.edu.cn/simple åŒ…å ( ä¸´æ—¶ä½¿ç”¨æ¸…åæº )
 - ç¯å¢ƒé…ç½®
     - https://blog.csdn.net/Tom197/article/details/119549236

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy åˆ›å»ºå·¥ç¨‹ç›®å½•
 - scrapy startproject é¡¹ç›®åç§°
 - cd åˆ°å·¥ç¨‹ç›®å½•ä¸­
 - scrapy genspider åå­— ç½‘å€
 - æ‰§è¡Œå·¥ç¨‹ scrapy crawl åå­—       æ‰€ä½¿ç”¨çš„æ˜¯ä¸Šé¢çš„åç§°
 - è®°å¾—åœ¨ setting.py ä¸­å°† å›å­åè®® è®¾ç½®ä¸º False
     - LOG_LEVEL = "ERROR" åªè¾“å‡ºé”™è¯¯å†…å®¹

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy ä»£ç åˆ†æ

class FirstchangeSpider(scrapy.Spider):
    name = 'firstChange'                        é¡¹ç›®åç§°, çˆ¬è™«æºæ–‡ä»¶çš„å”¯ä¸€æ ‡è¯† scrapy crawl firstChange
    # allowed_domains = ['qingju.eu.org']         å…è®¸çš„åŸŸå, åªçˆ¬å–å½“å‰åŸŸåä¸‹çš„å›¾ç‰‡ä¹‹ç±»çš„, ä¸€èˆ¬æ³¨é‡Šæ‰
    start_urls = ['http://qingju.eu.org/']      è¦çˆ¬å–çš„ç½‘ç«™, å¯ä»¥è®¾ç½®å¤šä¸ª

    def parse(self, response):                  ä¸Šé¢ start_urls è®¾ç½®å¤šå°‘ä¸ªè¿™é‡Œä¼šè°ƒç”¨å¤šå°‘æ¬¡
        pass

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy è§£æ




























